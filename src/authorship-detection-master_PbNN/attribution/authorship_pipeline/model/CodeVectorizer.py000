import torch
from torch import nn
from torch.nn import functional as F
from model.randwidth_ops import RWLinear, RWConv2d, RWBatchNorm2d

# Implementation of code2vec's vectorization part in PyTorch.
# Since it is a PyTorch Module, it can be reused as a part of another pipeline.

class CodeVectorizer(nn.Module):

    def __init__(self, n_tokens, n_paths, dim):
        super(CodeVectorizer, self).__init__()
        self.tokens_embed = nn.Embedding(n_tokens, dim)
        self.paths_embed = nn.Embedding(n_paths, dim)
        self.dropout = nn.Dropout(p=0.2)
        self.transform = nn.Sequential(nn.Linear(3 * dim, dim,False), nn.Tanh())    
        self.attention = nn.Linear(dim, 1) 
        self.trans_cache={}
        self.attes_cache={}

    def forward(self, contexts,new_dim=128):
        starts, paths, ends = contexts

        # (batch_size, max_contexts, dim)
        starts = self.tokens_embed(starts)
        paths = self.paths_embed(paths)
        ends = self.tokens_embed(ends)
        concatenated_contexts = torch.cat((starts, paths, ends), dim=2)
        #cat张量的拼接
        #print(concatenated_contexts.shape)
        #是这个耗内存吗？
        #尝试用dropout
        concatenated_contexts = self.dropout(concatenated_contexts)
        #print(concatenated_contexts.shape)
        
        # (batch_size, max_contexts, dim)

        #self.transform = nn.Sequential(RWLinear(3 * 128, new_dim,us=[True, False]), nn.Tanh())
        #self.transform = nn.Sequential(nn.Linear(3 * 128, new_dim,False), nn.Tanh())
        transformed_contexts = self.transform(concatenated_contexts)
        context_attentions = F.softmax(self.attention(transformed_contexts), dim=1)
        #print(transformed_contexts.shape)
        #print("self.trans_cache is "+str(self.trans_cache))
  
        
        # new_trans = None
        # if new_dim in self.trans_cache.keys():
        #     new_trans = self.trans_cache[new_dim]
        # else:
        #     new_trans = nn.Linear(128,new_dim)
        #     self.trans_cache[new_dim] = new_trans

        # transformed_contexts=new_trans(transformed_contexts)
        # # (batch_size, max_contexts, 1)
        
        # new_attention=None
        # if new_dim in self.attes_cache.keys():
        #     new_attention = self.attes_cache[new_dim]
        # else:
        #     new_attention = nn.Linear(new_dim, 1)
        #     self.attes_cache[new_dim] = new_attention
        
        # context_attentions = F.softmax(new_attention(transformed_contexts), dim=1)
        

        #print(context_attentions.shape)
        
        # (batch_size, dim)
        aggregated_context = torch.sum(torch.mul(transformed_contexts, context_attentions), dim=1)
        #print(aggregated_context.shape)
        return aggregated_context
        
'''        

class CodeVectorizer(nn.Module):

    def __init__(self, n_tokens, n_paths, dim):
        super(CodeVectorizer, self).__init__()
        self.tokens_embed = nn.Embedding(n_tokens, dim)
        self.paths_embed = nn.Embedding(n_paths, dim)
        #self.dropout = nn.Dropout(p=0.2)
        self.transform = nn.Sequential(nn.Linear(3 * dim, dim), nn.Tanh())
        self.attention = nn.Linear(dim, 1)

    def forward(self, contexts):
        starts, paths, ends = contexts

        # (batch_size, max_contexts, dim)
        starts = self.tokens_embed(starts)
        paths = self.paths_embed(paths)
        ends = self.tokens_embed(ends)

        # (batch_size, max_contexts, 3 * dim)
        concatenated_contexts = torch.cat((starts, paths, ends), dim=2)
        #cat张量的拼接
        #print(concatenated_contexts.shape)
        
        #concatenated_contexts = self.dropout(concatenated_contexts)
        #print(concatenated_contexts.shape)
        
        # (batch_size, max_contexts, dim)
        transformed_contexts = self.transform(concatenated_contexts)
        #print(transformed_contexts.shape)
        
        # (batch_size, max_contexts, 1)
        context_attentions = F.softmax(self.attention(transformed_contexts), dim=1)
        #print(context_attentions.shape)
        
        # (batch_size, dim)
        aggregated_context = torch.sum(torch.mul(transformed_contexts, context_attentions), dim=1)
        #print(aggregated_context.shape)
        return aggregated_context
'''